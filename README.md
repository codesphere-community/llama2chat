# llama.cpp

Forked from [llama.cpp](https://github.com/ggerganov/llama.cpp) to run Llama2 inference inside of [Codesphere](https://codesphere.com).  

The CI pipeline is configured to fetch a pre-converted and quantized llama code instruct model from TheBloke from [huggingface](https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF) and run the http server example, README with config options can be found in the /examples/server directory. 